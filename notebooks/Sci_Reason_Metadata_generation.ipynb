{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb3188a-c344-4a90-874d-a133fa1a6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extract figure discription\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import tarfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "def extract_tarfile(tar_path: str) -> str:\n",
    "    \"\"\"Extract tar.gz file to temporary directory.\"\"\"\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    try:\n",
    "        with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "            # Security check for path traversal\n",
    "            def is_safe_path(path):\n",
    "                return not (path.startswith('/') or '..' in path)\n",
    "            \n",
    "            members = [m for m in tar.getmembers() if is_safe_path(m.name)]\n",
    "            tar.extractall(path=temp_dir, members=members)\n",
    "    except tarfile.TarError as e:\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "        raise Exception(f\"Failed to extract tar file: {e}\")\n",
    "    return temp_dir\n",
    "\n",
    "def find_main_tex_file(folder: str) -> str:\n",
    "    \"\"\"Find the main LaTeX file, prioritizing common main file names.\"\"\"\n",
    "    # Common main file names (prioritized)\n",
    "    priority_names = ['main.tex', 'paper.tex', 'article.tex', 'manuscript.tex']\n",
    "    \n",
    "    # First, look for priority names\n",
    "    for priority in priority_names:\n",
    "        for root, _, files in os.walk(folder):\n",
    "            if priority in files:\n",
    "                return os.path.join(root, priority)\n",
    "    \n",
    "    # If no priority files found, return first .tex file\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.tex'):\n",
    "                return os.path.join(root, file)\n",
    "    \n",
    "    raise FileNotFoundError(f\"No .tex file found in: {folder}\")\n",
    "    \n",
    "def extract_figure_descriptions(tex_code: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Improved: Extract figure descriptions near \\includegraphics using larger context and paragraph heuristics.\n",
    "    \"\"\"\n",
    "    figure_descriptions = {}\n",
    "    pattern = r\"(\\\\begin\\{figure\\}.*?\\\\includegraphics(?:\\[[^\\]]*\\])?\\{(.+?)\\}.*?\\\\end\\{figure\\})\"\n",
    "\n",
    "    for match in re.finditer(pattern, tex_code, re.S):\n",
    "        full_block = match.group(1)\n",
    "        fig_path = match.group(2).strip()\n",
    "\n",
    "        # Expand context window\n",
    "        start = max(0, match.start() - 5000)\n",
    "        end = match.end() + 5000\n",
    "        context = tex_code[start:end]\n",
    "\n",
    "        # Look for paragraphs mentioning \"Figure\", \"Fig.\", etc.\n",
    "        paragraphs = re.split(r\"\\n\\s*\\n\", context)\n",
    "        candidate_paragraphs = [\n",
    "            p.strip()\n",
    "            for p in paragraphs\n",
    "            if (\n",
    "                \"figure\" in p.lower() or \"fig.\" in p.lower()\n",
    "            ) and len(p.strip()) > 40 and \"\\\\includegraphics\" not in p\n",
    "        ]\n",
    "\n",
    "        # If none found, fall back to any nearby paragraph\n",
    "        if not candidate_paragraphs:\n",
    "            candidate_paragraphs = [\n",
    "                p.strip()\n",
    "                for p in paragraphs\n",
    "                if len(p.strip()) > 40 and \"\\\\includegraphics\" not in p\n",
    "            ]\n",
    "\n",
    "        # Concatenate and truncate\n",
    "        full_desc = \" \".join(candidate_paragraphs)\n",
    "        figure_descriptions[fig_path] = full_desc[:1500]\n",
    "\n",
    "    return figure_descriptions\n",
    "\n",
    "\n",
    "def extract_sections(tex_code: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract methodology and conclusion sections from LaTeX code.\"\"\"\n",
    "    sections = {}\n",
    "    \n",
    "    # More comprehensive regex patterns\n",
    "    section_patterns = [\n",
    "        r'\\\\section\\*?\\{(.+?)\\}(.*?)(?=\\\\section|\\Z)',\n",
    "        r'\\\\subsection\\*?\\{(.+?)\\}(.*?)(?=\\\\(?:sub)?section|\\Z)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in section_patterns:\n",
    "        matches = re.findall(pattern, tex_code, re.S | re.I)\n",
    "        for title, content in matches:\n",
    "            title_lower = title.lower().strip()\n",
    "            \n",
    "            # keyword matching\n",
    "            if any(key in title_lower for key in ['method', 'model', 'approach', 'algorithm', 'technique']):\n",
    "                if 'methodology' not in sections or len(content.strip()) > len(sections.get('methodology', '')):\n",
    "                    sections['methodology'] = content.strip()\n",
    "            elif any(key in title_lower for key in ['conclusion', 'concluding', 'summary', 'discussion']):\n",
    "                if 'conclusion' not in sections or len(content.strip()) > len(sections.get('conclusion', '')):\n",
    "                    sections['conclusion'] = content.strip()\n",
    "            elif any(key in title_lower for key in ['introduction', 'intro']):\n",
    "                sections['introduction'] = content.strip()\n",
    "            elif any(key in title_lower for key in ['result', 'experiment', 'evaluation']):\n",
    "                sections['results'] = content.strip()\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def extract_figures(tex_code: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Extract figure information from LaTeX code.\"\"\"\n",
    "    figures = []\n",
    "    \n",
    "    # Multiple patterns to catch different figure formats\n",
    "    patterns = [\n",
    "        r'\\\\begin{figure}.*?\\\\includegraphics(?:\\[[^\\]]*\\])?\\{(.+?)\\}.*?\\\\caption\\{(.+?)\\}.*?\\\\end{figure}',\n",
    "        r'\\\\includegraphics(?:\\[[^\\]]*\\])?\\{(.+?)\\}.*?\\\\caption\\{(.+?)\\}',\n",
    "        r'\\\\begin{figure}.*?\\\\caption\\{(.+?)\\}.*?\\\\includegraphics(?:\\[[^\\]]*\\])?\\{(.+?)\\}.*?\\\\end{figure}',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, tex_code, re.S | re.I)\n",
    "        for match in matches:\n",
    "            if len(match) == 2:\n",
    "                img_file, caption = match\n",
    "                # Handle reversed order for some patterns\n",
    "                if pattern.endswith('includegraphics(?:\\\\[[^\\\\]]*\\\\])?\\\\{(.+?)\\\\}.*?\\\\\\\\end{figure}'):\n",
    "                    caption, img_file = match\n",
    "                \n",
    "                figures.append({\n",
    "                    'image_file': img_file.strip(),\n",
    "                    'caption': caption.strip()\n",
    "                })\n",
    "    \n",
    "    # Remove duplicates based on image file\n",
    "    seen = set()\n",
    "    unique_figures = []\n",
    "    for fig in figures:\n",
    "        if fig['image_file'] not in seen:\n",
    "            seen.add(fig['image_file'])\n",
    "            unique_figures.append(fig)\n",
    "    \n",
    "    return unique_figures\n",
    "\n",
    "def clean_latex(text: str) -> str:\n",
    "    \"\"\"Clean LaTeX commands and formatting from text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove comments\n",
    "    text = re.sub(r'%.*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove specific LaTeX commands but keep their content\n",
    "    text = re.sub(r'\\\\(?:textbf|textit|emph|texttt)\\{([^}]*)\\}', r'\\1', text)\n",
    "    text = re.sub(r'\\\\(?:cite|ref|label)\\{[^}]*\\}', '', text)\n",
    "    \n",
    "    # Remove math environments\n",
    "    text = re.sub(r'\\$+[^$]*\\$+', '', text)\n",
    "    text = re.sub(r'\\\\begin\\{equation\\}.*?\\\\end\\{equation\\}', '', text, flags=re.S)\n",
    "    text = re.sub(r'\\\\begin\\{align\\}.*?\\\\end\\{align\\}', '', text, flags=re.S)\n",
    "    \n",
    "    # Remove other LaTeX commands\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\*?\\{[^}]*\\}', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\*?(?:\\[[^\\]]*\\])?', '', text)\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def find_image_file(base_path: str, image_name: str) -> Optional[str]:\n",
    "    \"\"\"Find image file with various extensions and in subdirectories.\"\"\"\n",
    "    base_dir = os.path.dirname(base_path)\n",
    "    \n",
    "    # Remove extension if present\n",
    "    img_name_no_ext = os.path.splitext(image_name)[0]\n",
    "    \n",
    "    # Common image extensions\n",
    "    extensions = ['.png', '.jpg', '.jpeg', '.pdf', '.eps', '.svg', '.tif', '.tiff']\n",
    "    \n",
    "    # Search locations: same dir, figures/, images/, graphics/\n",
    "    search_dirs = [\n",
    "        base_dir,\n",
    "        os.path.join(base_dir, 'figures'),\n",
    "        os.path.join(base_dir, 'images'),\n",
    "        os.path.join(base_dir, 'graphics'),\n",
    "        os.path.join(base_dir, 'fig'),\n",
    "    ]\n",
    "    \n",
    "    for search_dir in search_dirs:\n",
    "        if not os.path.exists(search_dir):\n",
    "            continue\n",
    "            \n",
    "        # Try with original name\n",
    "        for ext in [''] + extensions:\n",
    "            candidate = os.path.join(search_dir, image_name + ext)\n",
    "            if os.path.exists(candidate):\n",
    "                return candidate\n",
    "        \n",
    "        # Try without extension + new extension\n",
    "        for ext in extensions:\n",
    "            candidate = os.path.join(search_dir, img_name_no_ext + ext)\n",
    "            if os.path.exists(candidate):\n",
    "                return candidate\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_tar_gz(tar_path: str, image_output_dir: str = 'dataset/images') -> List[Dict]:\n",
    "    \"\"\"Process a tar.gz file containing a LaTeX paper.\"\"\"\n",
    "    output_records = []\n",
    "    paper_id = Path(tar_path).stem.replace('.tar', '').replace('.gz', '')\n",
    "    \n",
    "    print(f\"🔍 Processing: {paper_id}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract tar file\n",
    "        tex_folder = extract_tarfile(tar_path)\n",
    "        os.makedirs(image_output_dir, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # Find and read main tex file\n",
    "            tex_file = find_main_tex_file(tex_folder)\n",
    "            print(f\"📄 Found TeX file: {os.path.basename(tex_file)}\")\n",
    "            \n",
    "            with open(tex_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                tex_code = f.read()\n",
    "            # Extract figure_descriptions \n",
    "            figure_descriptions = extract_figure_descriptions(tex_code)\n",
    "            # Extract sections and figures\n",
    "            sections = extract_sections(tex_code)\n",
    "            figures = extract_figures(tex_code)\n",
    "            \n",
    "            print(f\"📊 Found {len(figures)} figures, {len(sections)} sections\")\n",
    "            \n",
    "            # Process each figure\n",
    "            for i, fig in enumerate(figures):\n",
    "                image_name = fig['image_file']\n",
    "                \n",
    "                # Find the actual image file\n",
    "                image_path = find_image_file(tex_file, image_name)\n",
    "                \n",
    "                if image_path and os.path.exists(image_path):\n",
    "                    # Copy image to output directory\n",
    "                    new_img_name = f\"{paper_id}_fig{i+1}{Path(image_path).suffix}\"\n",
    "                    dest_path = os.path.join(image_output_dir, new_img_name)\n",
    "                    shutil.copyfile(image_path, dest_path)\n",
    "                    img_relative_path = os.path.join('images1', new_img_name)\n",
    "                    print(f\"✅ Copied: {new_img_name}\")\n",
    "                else:\n",
    "                    print(f\"⚠️  Image not found: {image_name}\")\n",
    "                    img_relative_path = None\n",
    "                \n",
    "                # Create record\n",
    "                record = {\n",
    "                    'paper_id': paper_id,\n",
    "                    'figure_id': f\"fig{i+1}\",\n",
    "                    'figure_path': img_relative_path,\n",
    "                    'caption': clean_latex(fig['caption']),\n",
    "                    'figure_description': clean_latex(figure_descriptions.get(image_name, \"\")),\n",
    "                    'methodology': clean_latex(sections.get('methodology', '')),\n",
    "                    'conclusion': clean_latex(sections.get('conclusion', '')),\n",
    "                    'introduction': clean_latex(sections.get('introduction', '')),\n",
    "                    'results': clean_latex(sections.get('results', '')),\n",
    "                }\n",
    "                \n",
    "                if img_relative_path is not None:\n",
    "                    output_records.append(record)\n",
    "\n",
    "        \n",
    "        finally:\n",
    "            # Clean up temporary directory\n",
    "            shutil.rmtree(tex_folder, ignore_errors=True)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {paper_id}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"✅ Processed {paper_id}: {len(output_records)} records\")\n",
    "    return output_records\n",
    "\n",
    "def save_jsonl(records: List[Dict], output_file: str) -> None:\n",
    "    \"\"\"Save records to JSONL format.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"💾 Saved {len(records)} records to {output_file}\")\n",
    "\n",
    "def process_multiple_papers(tar_files: List[str], output_file: str, image_dir: str = 'dataset/images') -> None:\n",
    "    \"\"\"Process multiple tar.gz files and save to single JSONL file.\"\"\"\n",
    "    all_records = []\n",
    "    \n",
    "    for tar_file in tar_files:\n",
    "        if not os.path.exists(tar_file):\n",
    "            print(f\"⚠️  File not found: {tar_file}\")\n",
    "            continue\n",
    "        \n",
    "        records = process_tar_gz(tar_file, image_dir)\n",
    "        all_records.extend(records)\n",
    "    \n",
    "    save_jsonl(all_records, output_file)\n",
    "    print(f\"🎉 Total processed: {len(all_records)} records from {len(tar_files)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9ca48b7-fe65-44c4-9b12-b5401490cc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Found 2 .tar.gz files\n",
      "🚀 Processing latexfiles/arXiv-2501.00034v1.tar.gz\n",
      "🔍 Processing: arXiv-2501.00034v1\n",
      "📄 Found TeX file: main.tex\n",
      "📊 Found 3 figures, 4 sections\n",
      "⚠️  Image not found: figures/inference\n",
      "✅ Copied: arXiv-2501.00034v1_fig2.png\n",
      "✅ Copied: arXiv-2501.00034v1_fig3.png\n",
      "✅ Processed arXiv-2501.00034v1: 2 records\n",
      "✅ Extracted 2 figures\n",
      "🚀 Processing latexfiles/arXiv-2506.02796v1.tar.gz\n",
      "🔍 Processing: arXiv-2506.02796v1\n",
      "📄 Found TeX file: LSTM_BEKK_v2.tex\n",
      "📊 Found 4 figures, 4 sections\n",
      "✅ Copied: arXiv-2506.02796v1_fig1.png\n",
      "✅ Copied: arXiv-2506.02796v1_fig2.png\n",
      "✅ Copied: arXiv-2506.02796v1_fig3.png\n",
      "✅ Copied: arXiv-2506.02796v1_fig4.png\n",
      "✅ Processed arXiv-2506.02796v1: 4 records\n",
      "✅ Extracted 4 figures\n",
      "📝 Saving 6 total records...\n",
      "💾 Saved 6 records to dataset/finance_dataset5.jsonl\n",
      "🎉 All done.\n"
     ]
    }
   ],
   "source": [
    "# === Run Batch Process ===(with figure discriptions)\n",
    "if __name__ == '__main__':\n",
    "    tar_files = glob('latexfiles/*.tar.gz')\n",
    "    print(f\"📦 Found {len(tar_files)} .tar.gz files\")\n",
    "\n",
    "    all_records = []\n",
    "    for fpath in tar_files:\n",
    "        print(f\"🚀 Processing {fpath}\")\n",
    "        records = process_tar_gz(fpath, image_output_dir='dataset/images1')\n",
    "        print(f\"✅ Extracted {len(records)} figures\")\n",
    "        all_records.extend(records)\n",
    "\n",
    "    print(f\"📝 Saving {len(all_records)} total records...\")\n",
    "    save_jsonl(all_records, 'dataset/finance_dataset5.jsonl')\n",
    "    print(\"🎉 All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329562c-d012-4a82-831a-5cfd18bcc1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
